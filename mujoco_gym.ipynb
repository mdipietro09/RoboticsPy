{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91ab3c1-1426-4321-a39f-6cdfd823f91b",
   "metadata": {},
   "source": [
    "# Robotics with Python: MuJoCo & Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae8c9e-1ca3-4c28-92db-1983f73b2ec2",
   "metadata": {},
   "source": [
    "### 1-Humanoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebc0e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec626f0d-ed53-4a61-9dd0-3db68228373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mujoco\n",
    "#pip install gymnasium\n",
    "\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6518852-f8f7-47bc-9c3a-b04fb9009e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_position': np.float64(0.0016152704880662335),\n",
       " 'y_position': np.float64(-0.006376264384969079),\n",
       " 'tendon_length': array([ 0.0039346 , -0.00659782]),\n",
       " 'tendon_velocity': array([-0.00065977,  0.00110333]),\n",
       " 'distance_from_origin': np.float64(0.006577677877233183)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed678fa4-f2a3-4d06-ad04-70f2e0fa6b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.40551438e+00,  1.00249109e+00,  1.02295992e-03,  6.11636741e-03,\n",
       "        8.89343991e-03,  3.68799654e-03,  2.68748631e-03, -7.37148320e-03,\n",
       "       -1.27035634e-03, -3.83448487e-03,  4.97995726e-03, -9.27465782e-04,\n",
       "       -5.30276103e-03, -6.80604484e-04, -9.18529355e-03, -1.14950454e-03,\n",
       "       -7.01506517e-03,  3.62228355e-03,  8.04498421e-03, -5.30207130e-03,\n",
       "       -1.90562795e-03, -6.06947519e-05,  2.28593828e-05,  6.61675641e-03,\n",
       "        9.77447190e-03, -6.79145658e-03,  9.35076611e-03,  9.40259276e-03,\n",
       "        6.24602486e-03, -7.69409031e-03,  7.79339588e-03, -3.57860426e-03,\n",
       "        8.84452451e-03,  2.14833940e-03,  3.15900130e-03, -3.69123743e-03,\n",
       "        5.47496853e-03,  3.01387670e-03,  9.26404279e-03, -1.41988753e-03,\n",
       "        1.43547743e-03, -1.56219701e-03,  6.16717799e-03,  5.88406125e-03,\n",
       "        6.06353189e-03,  2.30359698e+00,  2.28564819e+00,  4.23921394e-02,\n",
       "        3.81892603e-04,  4.27251199e-02, -1.34221053e-03, -9.78046855e-02,\n",
       "        3.05445261e-03,  4.35635947e+00,  8.90746237e+00,  9.54401200e-02,\n",
       "        9.04533260e-02,  1.09705672e-02,  1.69755818e-04,  9.77440278e-03,\n",
       "       -3.54790130e-04, -5.02470996e-02,  1.75551967e-03,  4.39996214e-01,\n",
       "        2.26194671e+00,  5.81292177e-02,  4.29646789e-02,  6.51220250e-02,\n",
       "        5.80889417e-04,  8.63263977e-03,  1.58756505e-04, -2.91065012e-01,\n",
       "       -4.52640715e-04,  1.96299323e-01,  6.61619413e+00,  2.72599949e-01,\n",
       "        2.31594969e-01,  5.48173694e-02, -1.10165227e-02, -2.15081603e-02,\n",
       "       -8.05523997e-02, -1.14986334e-01, -4.56016698e-01, -8.55144168e-01,\n",
       "        4.75175093e+00,  9.30058281e-01,  9.08257704e-01,  2.96113625e-02,\n",
       "       -7.45843650e-03, -4.52459751e-02, -1.45304611e-01, -7.97117497e-02,\n",
       "       -2.57757649e-01, -1.55134593e+00,  2.75569617e+00,  1.04849763e+00,\n",
       "        1.03433793e+00,  2.16319803e-02, -5.27336804e-03, -4.24042258e-02,\n",
       "       -1.27917885e-01, -5.55800809e-02, -1.67664573e-01, -1.34822496e+00,\n",
       "        1.76714587e+00,  2.73620069e-01,  2.33731863e-01,  5.44965578e-02,\n",
       "        1.17204654e-02, -2.20841756e-02,  7.98811510e-02, -1.23115710e-01,\n",
       "        4.52137460e-01, -8.59998452e-01,  4.75175093e+00,  9.31565346e-01,\n",
       "        9.10875724e-01,  2.73534715e-02,  6.24128059e-03, -3.89845480e-02,\n",
       "        1.40209730e-01, -6.91843688e-02,  2.48597865e-01, -1.55417260e+00,\n",
       "        2.75569617e+00,  1.04980295e+00,  1.03650562e+00,  1.94597503e-02,\n",
       "        3.96627251e-03, -3.35788530e-02,  1.21831372e-01, -4.39522220e-02,\n",
       "        1.59468208e-01, -1.35007352e+00,  1.76714587e+00,  4.26043644e-01,\n",
       "        3.33216404e-01,  1.23120699e-01,  3.61190658e-02, -4.96240275e-02,\n",
       "        1.74239507e-01,  1.25795200e-01, -4.12812743e-01,  7.20503636e-01,\n",
       "        1.66108048e+00,  3.17992102e-01,  3.49033327e-01,  1.76170230e-01,\n",
       "        7.75444392e-02, -1.58742037e-01,  1.23212715e-01,  3.49135689e-01,\n",
       "       -2.88453019e-01,  5.44136086e-01,  1.22954019e+00,  4.28347842e-01,\n",
       "        3.30492074e-01,  1.23123359e-01, -3.26157175e-02, -4.31926088e-02,\n",
       "       -1.75926261e-01,  1.10728637e-01,  4.17346697e-01,  7.20123919e-01,\n",
       "        1.66108048e+00,  3.21184385e-01,  3.41993031e-01,  1.77012662e-01,\n",
       "       -7.87837637e-02, -1.54162709e-01, -1.28254775e-01,  3.39987737e-01,\n",
       "        3.00797120e-01,  5.41540989e-01,  1.22954019e+00, -6.84076360e-03,\n",
       "        9.21053669e-03,  9.50470552e-03, -4.15787824e-03,  3.59272395e-03,\n",
       "        9.69593440e-03, -6.62465076e-03,  1.50571617e-03,  1.57337414e-02,\n",
       "       -2.15429938e-03,  3.78276537e-03,  9.86148745e-03,  1.16677185e-03,\n",
       "        1.67272721e-03,  1.56802586e-02, -2.17597989e-03,  4.79072981e-03,\n",
       "        9.85062665e-03, -2.38936807e-03,  3.80244570e-03,  2.45350083e-02,\n",
       "       -3.03875599e-03,  5.02198138e-03,  9.44850876e-03, -2.33364155e-03,\n",
       "        6.44002482e-04,  2.45555066e-02, -4.28184579e-03,  5.00063106e-03,\n",
       "        9.53823858e-03, -2.33364155e-03,  6.44002482e-04,  2.45555066e-02,\n",
       "       -4.28184579e-03,  5.00063106e-03,  9.53823858e-03,  4.75286113e-03,\n",
       "        4.76476840e-03,  1.01804342e-02, -2.69266720e-03,  4.60606018e-03,\n",
       "        9.40990477e-03,  4.95768904e-03, -4.49700968e-03,  1.01793688e-02,\n",
       "       -6.34177039e-03,  4.52533398e-03,  9.62580020e-03,  4.95768904e-03,\n",
       "       -4.49700968e-03,  1.01793688e-02, -6.34177039e-03,  4.52533398e-03,\n",
       "        9.62580020e-03, -7.97208252e-03,  7.60038901e-03,  9.95661037e-03,\n",
       "       -3.40690821e-03,  3.01353006e-03,  9.51226499e-03, -7.99880208e-03,\n",
       "        8.70075877e-03,  8.84803833e-03, -3.38899939e-03,  3.20060734e-03,\n",
       "        9.69752644e-03, -1.73650502e-03,  1.09479898e-02,  1.61065282e-02,\n",
       "       -3.92784290e-03,  6.29366210e-03,  8.80725357e-03, -1.73088570e-03,\n",
       "        6.65053052e-03,  1.18288850e-02, -4.00856688e-03,  6.99671144e-03,\n",
       "        8.10084133e-03,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aff0a3e-a047-4da7-aead-28470aad0f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-0.4, 0.4, (17,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d19bf1-44ae-4628-909d-8a529b9077e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05293579, -0.02909377,  0.12628955,  0.15863812, -0.14441639,\n",
       "       -0.0146535 , -0.04183137,  0.30172417, -0.05358713, -0.20124765,\n",
       "       -0.3164999 , -0.08170177, -0.13806766, -0.0086144 , -0.1124486 ,\n",
       "       -0.22309826,  0.36465538], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "865db760-7704-44a3-a5b8-7717a332cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 1 - Step:100, Reward:4.7, Total:467.3\n",
      "EPISODE 1 - Step:200, Reward:4.9, Total:951.0\n"
     ]
    }
   ],
   "source": [
    "# RANDOM ACTIONS\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if the humanoid falls or the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(240):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action = env.action_space.sample() #random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() #render physics step (CPU speed = 0.1 seconds)\n",
    "    time.sleep(1/240) #slow down to real-time (240 steps × 1/240 second sleep = 1 second)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05289fe9-8ac4-4158-b76f-23f4c9cf3b64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58dc0b4-5b6e-402d-b8e6-c732aba106bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WindowViewer.__del__ at 0x7fd4ce8e15f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 335, in __del__\n",
      "    self.free()\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 330, in free\n",
      "    glfw.destroy_window(self.window)\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/glfw/__init__.py\", line 1282, in destroy_window\n",
      "    window_addr = ctypes.cast(ctypes.pointer(window),\n",
      "TypeError: _type_ must have storage info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:4.7, Total:4.7\n",
      "EPISODE 1 - Step:20, Reward:5.6, Total:96.8\n",
      "------------------------------------------\n",
      "EPISODE 2 - Step:1, Reward:4.6, Total:4.6\n",
      "EPISODE 2 - Step:37, Reward:5.0, Total:187.5\n",
      "------------------------------------------\n",
      "EPISODE 3 - Step:1, Reward:4.7, Total:4.7\n",
      "EPISODE 3 - Step:53, Reward:5.9, Total:279.8\n",
      "------------------------------------------\n",
      "EPISODE 4 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 4 - Step:33, Reward:6.2, Total:179.3\n",
      "------------------------------------------\n",
      "EPISODE 5 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 5 - Step:37, Reward:6.2, Total:203.6\n",
      "------------------------------------------\n",
      "EPISODE 6 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 6 - Step:42, Reward:6.2, Total:230.4\n",
      "------------------------------------------\n",
      "EPISODE 7 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 7 - Step:41, Reward:6.3, Total:224.2\n",
      "------------------------------------------\n",
      "EPISODE 8 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 8 - Step:34, Reward:6.2, Total:186.7\n",
      "------------------------------------------\n",
      "EPISODE 9 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 9 - Step:38, Reward:6.3, Total:211.8\n",
      "------------------------------------------\n",
      "EPISODE 10 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 10 - Step:36, Reward:6.2, Total:197.7\n",
      "------------------------------------------\n",
      "EPISODE 11 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 11 - Step:42, Reward:5.9, Total:217.9\n",
      "------------------------------------------\n",
      "EPISODE 12 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 12 - Step:34, Reward:6.1, Total:182.4\n",
      "------------------------------------------\n",
      "EPISODE 13 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 13 - Step:34, Reward:6.3, Total:186.9\n",
      "------------------------------------------\n",
      "EPISODE 14 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 14 - Step:38, Reward:6.4, Total:211.3\n",
      "------------------------------------------\n",
      "EPISODE 15 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 15 - Step:36, Reward:6.4, Total:200.9\n",
      "------------------------------------------\n",
      "EPISODE 16 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 16 - Step:37, Reward:6.4, Total:204.9\n",
      "------------------------------------------\n",
      "EPISODE 17 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 17 - Step:37, Reward:6.4, Total:204.3\n",
      "------------------------------------------\n",
      "EPISODE 18 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 18 - Step:37, Reward:6.2, Total:201.1\n",
      "------------------------------------------\n",
      "EPISODE 19 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 19 - Step:36, Reward:6.1, Total:193.5\n",
      "------------------------------------------\n",
      "EPISODE 20 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 20 - Step:42, Reward:6.2, Total:230.0\n",
      "------------------------------------------\n",
      "EPISODE 21 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 21 - Step:39, Reward:6.2, Total:213.2\n",
      "------------------------------------------\n",
      "EPISODE 22 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 22 - Step:40, Reward:6.2, Total:217.3\n",
      "------------------------------------------\n",
      "EPISODE 23 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 23 - Step:50, Reward:5.7, Total:257.6\n",
      "------------------------------------------\n",
      "EPISODE 24 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 24 - Step:40, Reward:5.1, Total:198.2\n",
      "------------------------------------------\n",
      "EPISODE 25 - Step:1, Reward:4.9, Total:4.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_z/cl6cx94516g2tlbhzcbl4nfc0000gn/T/ipykernel_63924/848025175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mexploration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add random noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_action\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m## reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/humanoid_v4.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         return self.mujoco_renderer.render(\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         )\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, render_mode, camera_id, camera_name)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_viewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                 \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    392\u001b[0m                     )\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mglfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m             \u001b[0mglfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             self._time_per_render = 0.9 * self._time_per_render + 0.1 * (\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/glfw/__init__.py\u001b[0m in \u001b[0;36mswap_buffers\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   2378\u001b[0m         \u001b[0mvoid\u001b[0m \u001b[0mglfwSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLFWwindow\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m     \"\"\"\n\u001b[0;32m-> 2380\u001b[0;31m     \u001b[0m_glfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglfwSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m \u001b[0m_glfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglfwSwapInterval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/glfw/__init__.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, *args)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0m_callback_exception_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \"\"\"\n\u001b[0;32m--> 687\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_exc_info_from_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_exc_info_from_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = True #reset if the humanoid falls or the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "exploration_rate = 0.5 #start wild\n",
    "preferred_action = np.zeros(env.action_space.shape) #knowledge to update with experience\n",
    "\n",
    "for _ in range(1000):\n",
    "    ## action\n",
    "    step += 1\n",
    "    exploration = np.random.normal(loc=0, scale=exploration_rate, size=env.action_space.shape) #add random noise\n",
    "    action = np.clip(a=preferred_action+exploration, a_min=-1, a_max=1)\n",
    "    obs, reward, terminated, truncated, info = env.step(action) \n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    if reward > 0:\n",
    "        preferred_action += (action-preferred_action)*0.05 #learning_rate\n",
    "    exploration_rate = max(0.05, exploration_rate*0.99) #min_exploration=0.05, decay_exploration=0.99\n",
    "    ## render\n",
    "    env.render() \n",
    "    time.sleep(1/240)\n",
    "    if (step == 1) or (step % 100 == 0):\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated:\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8942f3a-53fc-4cb3-b1e5-f762ef5fc827",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb8d555-7b8d-4aff-9575-67d36e392ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WindowViewer.__del__ at 0x7fef22e7e0e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 335, in __del__\n",
      "    self.free()\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 330, in free\n",
      "    glfw.destroy_window(self.window)\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/glfw/__init__.py\", line 1282, in destroy_window\n",
      "    window_addr = ctypes.cast(ctypes.pointer(window),\n",
      "TypeError: _type_ must have storage info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DONE\n"
     ]
    }
   ],
   "source": [
    "#pip install torch\n",
    "#pip install stable-baselines3\n",
    "#pip install tensorboard\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# TRAIN\n",
    "env = gym.make(\"Humanoid-v4\") #no rendering to speed up\n",
    "env = DummyVecEnv([lambda:env])\n",
    "\n",
    "print(\"Training START\")\n",
    "model = PPO(policy=\"MlpPolicy\", env=env, verbose=0, \n",
    "            learning_rate=0.005, ent_coef=0.005, #exploration\n",
    "            tensorboard_log=\"logs/\") #>tensorboard --logdir=logs/\n",
    "model.learn(total_timesteps=3_000_000, #1h\n",
    "            tb_log_name=\"model_humanoid\", log_interval=10)\n",
    "print(\"Training DONE\")\n",
    "\n",
    "model.save(\"model_humanoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a132979e-2f8d-4cb6-a905-0071520f5f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:4.7, Total:4.7\n",
      "EPISODE 1 - Step:100, Reward:5.8, Total:509.3\n",
      "EPISODE 1 - Step:200, Reward:4.6, Total:1014.7\n",
      "EPISODE 1 - Step:300, Reward:4.9, Total:1478.7\n",
      "EPISODE 1 - Step:400, Reward:4.6, Total:1941.6\n",
      "EPISODE 1 - Step:500, Reward:4.7, Total:2404.6\n",
      "EPISODE 1 - Step:600, Reward:4.7, Total:2879.2\n",
      "EPISODE 1 - Step:700, Reward:4.6, Total:3344.9\n",
      "EPISODE 1 - Step:800, Reward:4.5, Total:3816.8\n",
      "EPISODE 1 - Step:900, Reward:4.7, Total:4287.6\n",
      "EPISODE 1 - Step:1000, Reward:4.5, Total:4749.2\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\n",
    "model = PPO.load(path=\"model_humanoid\", env=env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if the humanoid falls or the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action, _ = model.predict(obs)    \n",
    "    obs, reward, terminated, truncated, info = env.step(action) \n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() \n",
    "    time.sleep(1/240)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b667fe",
   "metadata": {},
   "source": [
    "### 2-Ant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a1866",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "866bec37-65ed-42c0-bc59-902f557ddc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFO: 0 ---\n",
      "{} \n",
      "\n",
      "--- OBS: (27,) ---\n",
      "[ 0.69610069  0.99107476 -0.05122793  0.09623242 -0.07671926 -0.01977059\n",
      " -0.02540361  0.00562458 -0.04195053 -0.02365713  0.02634761  0.01637139\n",
      " -0.06810819  0.18836808  0.01321965 -0.03006503  0.01513391 -0.00329373\n",
      " -0.03492774  0.03013392  0.19484554 -0.04195543 -0.11086913  0.05685866\n",
      "  0.06538731 -0.02025416  0.15936732] \n",
      "\n",
      "--- ACTIONS: Box(-1.0, 1.0, (8,), float32) ---\n",
      "[-0.24314626  0.5464993   0.23927115  0.1269832   0.7842774  -0.6511408\n",
      " -0.8717369  -0.27321705] \n",
      "\n",
      "--- REWARD ---\n",
      "0.88188261885989 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"Ant-v4\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(f\"--- INFO: {len(info)} ---\")\n",
    "print(info, \"\\n\")\n",
    "\n",
    "print(f\"--- OBS: {obs.shape} ---\")\n",
    "print(obs, \"\\n\")\n",
    "\n",
    "print(f\"--- ACTIONS: {env.action_space} ---\")\n",
    "print(env.action_space.sample(), \"\\n\")\n",
    "\n",
    "print(f\"--- REWARD ---\")\n",
    "obs, reward, terminated, truncated, info = env.step( env.action_space.sample() )\n",
    "print(reward, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203263a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:-0.2, Total:-0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/glfw/__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:100, Reward:-0.1, Total:-29.6\n",
      "EPISODE 1 - Step:200, Reward:1.8, Total:-8.8\n"
     ]
    }
   ],
   "source": [
    "# RANDOM ACTIONS\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Ant-v4\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(240):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action = env.action_space.sample() #random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() #render physics step (CPU speed = 0.1 seconds)\n",
    "    time.sleep(1/240) #slow down to real-time (240 steps × 1/240 second sleep = 1 second)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e098b5-7c9f-4377-b7cd-d675d9ee7ee6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Custom Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33d76da-b0ef-42df-9341-d2718786504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/gymnasium/envs/mujoco/assets/ant.xml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.join(os.path.dirname(gym.__file__), \"envs/mujoco/assets/ant.xml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214ab046-9d52-48bf-aa96-f7732f3d12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.mujoco.ant_v4 import AntEnv\n",
    "from gymnasium.envs.registration import register\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## modify the class\n",
    "class CustomAntEnv(AntEnv):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(xml_file=os.getcwd()+\"/assets/custom_ant.xml\", **kwargs)\n",
    "\n",
    "    def CUSTOM_REWARD(self, action, info):\n",
    "        torso_height = float(self.data.qpos[2]) #torso z-coordinate = how high it is\n",
    "        reward = np.clip(a=torso_height-0.6, a_min=0, a_max=1) *10 #when the torso is high\n",
    "        terminated = bool(torso_height < 0.2 ) #if torso close to the ground\n",
    "        info[\"torso_height\"] = torso_height #add info for logging\n",
    "        return reward, terminated, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action) #override original step()\n",
    "        new_reward, new_terminated, new_info = self.CUSTOM_REWARD(action, info)\n",
    "        return obs, new_reward, new_terminated, truncated, new_info #must return the same things\n",
    "\n",
    "    def reset_model(self):\n",
    "        return super().reset_model() #keeping the reset as it is\n",
    "\n",
    "## register the new env\n",
    "register(id=\"CustomAntEnv-v1\", entry_point=\"__main__:CustomAntEnv\")\n",
    "\n",
    "## test\n",
    "env = gym.make(\"CustomAntEnv-v1\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cda11-25e3-4f85-a0db-052db5445751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb521257-cf55-4547-8d9e-6921dff2dde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFO: 0 ---\n",
      "{} \n",
      "\n",
      "--- OBS: (27,) ---\n",
      "[ 0.80670198  0.99372207  0.05344449 -0.08964352 -0.04030102 -0.09079147\n",
      "  0.07141362  0.07548885  0.05107503 -0.01731285 -0.07544979 -0.01927543\n",
      "  0.02853044  0.06478572  0.10862236  0.06993676  0.05588387  0.04996524\n",
      "  0.05051555 -0.143312   -0.03659429 -0.12759017  0.07585792 -0.03603179\n",
      " -0.19314837 -0.03998299  0.00745016] \n",
      "\n",
      "--- ACTIONS: Discrete(5) ---\n",
      "discrete: 1 -> continuous: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] \n",
      "\n",
      "--- REWARD ---\n",
      "2.3082665225778154 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DISCRETIZE THE ENV\n",
    "class DiscreteEnvWrapper(gym.Env):\n",
    "    \n",
    "    def __init__(self, render_mode=None): \n",
    "        super().__init__() \n",
    "        self.env = gym.make(\"CustomAntEnv-v1\", render_mode=render_mode) \n",
    "        self.action_space = gym.spaces.Discrete(5)  #will have 5 actions \n",
    "        self.observation_space = self.env.observation_space #same observation space\n",
    "        n_joints = self.env.action_space.shape[0]         \n",
    "        self.action_map = [\n",
    "            ## action 0 = stand still \n",
    "            np.zeros(n_joints),\n",
    "            ## action 1 = push all forward\n",
    "            0.5*np.ones(n_joints),\n",
    "            ## action 2 = push all backward\n",
    "           -0.5*np.ones(n_joints),\n",
    "            ## action 3 = front legs forward + back legs backward \n",
    "            0.5*np.concatenate([np.ones(n_joints//2), -np.ones(n_joints//2)]),\n",
    "            ## action 4 = front legs backward + back legs forward \n",
    "            0.5*np.concatenate([-np.ones(n_joints//2), np.ones(n_joints//2)])\n",
    "        ] \n",
    "        \n",
    "    def step(self, discrete_action): \n",
    "        assert self.action_space.contains(discrete_action) \n",
    "        continuous_action = self.action_map[discrete_action] \n",
    "        obs, reward, terminated, truncated, info = self.env.step(continuous_action) \n",
    "        return obs, reward, terminated, truncated, info\n",
    "        \n",
    "    def reset(self, **kwargs): \n",
    "        obs, info = self.env.reset(**kwargs) \n",
    "        return obs, info \n",
    "    \n",
    "    def render(self): \n",
    "        return self.env.render() \n",
    "    \n",
    "    def close(self): \n",
    "        self.env.close()\n",
    "\n",
    "## test\n",
    "env = DiscreteEnvWrapper()\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(f\"--- INFO: {len(info)} ---\")\n",
    "print(info, \"\\n\")\n",
    "\n",
    "print(f\"--- OBS: {obs.shape} ---\")\n",
    "print(obs, \"\\n\")\n",
    "\n",
    "print(f\"--- ACTIONS: {env.action_space} ---\")\n",
    "discrete_action = env.action_space.sample()\n",
    "continuous_action = env.action_map[discrete_action] \n",
    "print(\"discrete:\", discrete_action, \"-> continuous:\", continuous_action, \"\\n\")\n",
    "\n",
    "print(f\"--- REWARD ---\")\n",
    "obs, reward, terminated, truncated, info = env.step( discrete_action )\n",
    "print(reward, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5751ce07-08ba-42cb-959b-8d6b47aefdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training START\n",
      "Training DONE\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# TRAIN\n",
    "env = DiscreteEnvWrapper(render_mode=None) #no rendering to speed up\n",
    "env = DummyVecEnv([lambda:env]) \n",
    "model_name = \"ant_dqn\"\n",
    "\n",
    "print(\"Training START\")\n",
    "model = sb.DQN(policy=\"MlpPolicy\", env=env, verbose=0, learning_rate=0.005,\n",
    "               exploration_fraction=0.2, exploration_final_eps=0.05, #eps decays linearly from 1 to 0.05\n",
    "               tensorboard_log=\"logs/\") #>tensorboard --logdir=logs/\n",
    "model.learn(total_timesteps=1_000_000, #1h\n",
    "            tb_log_name=model_name, log_interval=10)\n",
    "print(\"Training DONE\")\n",
    "\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9ebee11-d8eb-40cc-a11c-cbde4e135d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:0.9, Total:0.9\n",
      "EPISODE 1 - Step:100, Reward:0.0, Total:1.9\n",
      "EPISODE 1 - Step:200, Reward:0.0, Total:60.0\n",
      "EPISODE 1 - Step:300, Reward:0.0, Total:121.8\n",
      "EPISODE 1 - Step:400, Reward:0.0, Total:198.8\n",
      "EPISODE 1 - Step:500, Reward:0.0, Total:267.8\n",
      "EPISODE 1 - Step:600, Reward:0.0, Total:325.7\n",
      "EPISODE 1 - Step:700, Reward:0.0, Total:325.7\n",
      "EPISODE 1 - Step:800, Reward:2.8, Total:437.4\n",
      "EPISODE 1 - Step:900, Reward:0.0, Total:527.5\n",
      "EPISODE 1 - Step:1000, Reward:0.0, Total:527.5\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "env = DiscreteEnvWrapper(render_mode=\"human\")\n",
    "model = sb.DQN.load(path=model_name, env=env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action, _ = model.predict(obs)    \n",
    "    obs, reward, terminated, truncated, info = env.step(action) \n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() \n",
    "    time.sleep(1/240)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8d71c-f233-4bc3-a75c-d9f190b30fe9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "309d2cd6-5237-4839-b1b3-d1c5fbcf6045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training START\n",
      "Training DONE\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "env_name, model_name = \"CustomAntEnv-v1\", \"ant_sac\"\n",
    "env = gym.make(env_name) #no rendering to speed up\n",
    "env = DummyVecEnv([lambda:env])\n",
    "\n",
    "print(\"Training START\")\n",
    "model = sb.SAC(policy=\"MlpPolicy\", env=env, verbose=0, learning_rate=0.005, \n",
    "                ent_coef=0.005, #exploration\n",
    "                tensorboard_log=\"logs/\") #>tensorboard --logdir=logs/\n",
    "model.learn(total_timesteps=100_000, #3h\n",
    "            tb_log_name=model_name, log_interval=10)\n",
    "print(\"Training DONE\")\n",
    "\n",
    "## save\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ef8e07-337d-429d-a0ad-3aa315222caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WindowViewer.__del__ at 0x7fb2ef249bd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 335, in __del__\n",
      "    self.free()\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 330, in free\n",
      "    glfw.destroy_window(self.window)\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/glfw/__init__.py\", line 1282, in destroy_window\n",
      "    window_addr = ctypes.cast(ctypes.pointer(window),\n",
      "TypeError: _type_ must have storage info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:1.3, Total:1.3\n",
      "EPISODE 1 - Step:100, Reward:10.0, Total:785.3\n",
      "EPISODE 1 - Step:200, Reward:10.0, Total:1729.2\n",
      "EPISODE 1 - Step:300, Reward:4.3, Total:2412.0\n",
      "EPISODE 1 - Step:400, Reward:10.0, Total:3257.6\n",
      "EPISODE 1 - Step:500, Reward:4.2, Total:4059.2\n",
      "EPISODE 1 - Step:600, Reward:4.0, Total:4926.7\n",
      "EPISODE 1 - Step:700, Reward:10.0, Total:5802.7\n",
      "EPISODE 1 - Step:800, Reward:4.4, Total:6678.0\n",
      "EPISODE 1 - Step:900, Reward:5.4, Total:7430.4\n",
      "EPISODE 1 - Step:1000, Reward:10.0, Total:8311.7\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "model = sb.SAC.load(path=model_name, env=env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action, _ = model.predict(obs)    \n",
    "    obs, reward, terminated, truncated, info = env.step(action) \n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() \n",
    "    time.sleep(1/240)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f62013-4405-4c61-a57a-1293f93ed501",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Evolutionary Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "556165bb-30ac-454c-9a08-a6b7320c07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-09 23:57:42] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424893561728) -- The `dtype` for the problem's decision variables is set as torch.float32\n",
      "[2025-11-09 23:57:42] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424893561728) -- `eval_dtype` (the dtype of the fitnesses and evaluation data) is set as torch.float32\n",
      "[2025-11-09 23:57:42] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424893561728) -- The `device` of the problem is set as cpu\n",
      "[2025-11-09 23:57:42] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424893561728) -- The number of actors that will be allocated for parallelized evaluation is 8\n",
      "[2025-11-09 23:57:42] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424893561728) -- Number of GPUs that will be allocated per actor is None\n",
      "                   iter : 20\n",
      "              mean_eval : -29.074871063232422\n",
      "          pop_best_eval : 166.01072692871094\n",
      "            median_eval : -35.795536041259766\n",
      "              best_eval : 565.2426147460938\n",
      "             worst_eval : -197.5804901123047\n",
      "total_interaction_count : 80000\n",
      "    total_episode_count : 400\n",
      "\n",
      "                   iter : 40\n",
      "              mean_eval : -34.52744674682617\n",
      "          pop_best_eval : 275.5376281738281\n",
      "            median_eval : -75.46575164794922\n",
      "              best_eval : 871.6951904296875\n",
      "             worst_eval : -197.5804901123047\n",
      "total_interaction_count : 160000\n",
      "    total_episode_count : 800\n",
      "\n",
      "                   iter : 60\n",
      "              mean_eval : 53.383644104003906\n",
      "          pop_best_eval : 439.68927001953125\n",
      "            median_eval : 8.099838256835938\n",
      "              best_eval : 871.6951904296875\n",
      "             worst_eval : -197.5804901123047\n",
      "total_interaction_count : 240000\n",
      "    total_episode_count : 1200\n",
      "\n",
      "                   iter : 80\n",
      "              mean_eval : 81.7454605102539\n",
      "          pop_best_eval : 497.7518005371094\n",
      "            median_eval : -37.290950775146484\n",
      "              best_eval : 918.9232177734375\n",
      "             worst_eval : -197.9688720703125\n",
      "total_interaction_count : 320000\n",
      "    total_episode_count : 1600\n",
      "\n",
      "                   iter : 100\n",
      "              mean_eval : 110.3915023803711\n",
      "          pop_best_eval : 605.0982666015625\n",
      "            median_eval : 52.22205352783203\n",
      "              best_eval : 918.9232177734375\n",
      "             worst_eval : -197.9688720703125\n",
      "total_interaction_count : 400000\n",
      "    total_episode_count : 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install evotorch\n",
    "\n",
    "from evotorch.neuroevolution import GymNE\n",
    "from evotorch.algorithms import PGPE\n",
    "from evotorch.logging import StdOutLogger\n",
    "\n",
    "## problem\n",
    "train = GymNE(env=CustomAntEnv, #directly the class because it's custom env\n",
    "              env_config={\"render_mode\":None}, #no rendering to speed up\n",
    "              network=\"Linear(obs_length, act_length)\", #linear policy\n",
    "              observation_normalization=True,\n",
    "              decrease_rewards_by=1, #normalization trick to stabilize evolution\n",
    "              episode_length=200, #steps per episode\n",
    "              num_actors=\"max\") #use all available CPU cores\n",
    "\n",
    "## model\n",
    "model = PGPE(problem=train, popsize=20, stdev_init=0.1, #keep it small\n",
    "             center_learning_rate=0.005, stdev_learning_rate=0.1,\n",
    "             optimizer_config={\"max_speed\":0.015})\n",
    "\n",
    "## train\n",
    "StdOutLogger(searcher=model, interval=20)\n",
    "model.run(num_generations=100) #1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a249e4d5-e1ac-4abd-bde9-e19c4a36060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-10 00:04:22] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424981781216) -- The `dtype` for the problem's decision variables is set as torch.float32\n",
      "[2025-11-10 00:04:22] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424981781216) -- `eval_dtype` (the dtype of the fitnesses and evaluation data) is set as torch.float32\n",
      "[2025-11-10 00:04:22] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424981781216) -- The `device` of the problem is set as cpu\n",
      "[2025-11-10 00:04:22] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424981781216) -- The number of actors that will be allocated for parallelized evaluation was encountered as 1. This number is automatically dropped to 0, because having only 1 actor does not bring any benefit in terms of parallelization.\n",
      "[2025-11-10 00:04:22] INFO     <22215> evotorch.core: Instance of `GymNE` (id:140424981781216) -- The number of actors that will be allocated for parallelized evaluation is 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cumulative_reward': 50741.49503841763, 'interaction_count': 11865}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## visualization problem\n",
    "test = GymNE(env=CustomAntEnv, env_config={\"render_mode\":\"human\"},\n",
    "             network=\"Linear(obs_length, act_length)\",\n",
    "             observation_normalization=True,\n",
    "             decrease_rewards_by=1,\n",
    "             num_actors=1) #only need 1 for visualization\n",
    "\n",
    "## test best policy\n",
    "population_center = model.status[\"center\"]\n",
    "policy = test.to_policy(population_center)\n",
    "\n",
    "## render\n",
    "test.visualize(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
