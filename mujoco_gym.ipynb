{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91ab3c1-1426-4321-a39f-6cdfd823f91b",
   "metadata": {},
   "source": [
    "# Robotics with Python: MuJoCo & Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae8c9e-1ca3-4c28-92db-1983f73b2ec2",
   "metadata": {},
   "source": [
    "### 1-Humanoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebc0e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec626f0d-ed53-4a61-9dd0-3db68228373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mujoco\n",
    "#pip install gymnasium\n",
    "\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6518852-f8f7-47bc-9c3a-b04fb9009e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_position': np.float64(0.0016152704880662335),\n",
       " 'y_position': np.float64(-0.006376264384969079),\n",
       " 'tendon_length': array([ 0.0039346 , -0.00659782]),\n",
       " 'tendon_velocity': array([-0.00065977,  0.00110333]),\n",
       " 'distance_from_origin': np.float64(0.006577677877233183)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed678fa4-f2a3-4d06-ad04-70f2e0fa6b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.40551438e+00,  1.00249109e+00,  1.02295992e-03,  6.11636741e-03,\n",
       "        8.89343991e-03,  3.68799654e-03,  2.68748631e-03, -7.37148320e-03,\n",
       "       -1.27035634e-03, -3.83448487e-03,  4.97995726e-03, -9.27465782e-04,\n",
       "       -5.30276103e-03, -6.80604484e-04, -9.18529355e-03, -1.14950454e-03,\n",
       "       -7.01506517e-03,  3.62228355e-03,  8.04498421e-03, -5.30207130e-03,\n",
       "       -1.90562795e-03, -6.06947519e-05,  2.28593828e-05,  6.61675641e-03,\n",
       "        9.77447190e-03, -6.79145658e-03,  9.35076611e-03,  9.40259276e-03,\n",
       "        6.24602486e-03, -7.69409031e-03,  7.79339588e-03, -3.57860426e-03,\n",
       "        8.84452451e-03,  2.14833940e-03,  3.15900130e-03, -3.69123743e-03,\n",
       "        5.47496853e-03,  3.01387670e-03,  9.26404279e-03, -1.41988753e-03,\n",
       "        1.43547743e-03, -1.56219701e-03,  6.16717799e-03,  5.88406125e-03,\n",
       "        6.06353189e-03,  2.30359698e+00,  2.28564819e+00,  4.23921394e-02,\n",
       "        3.81892603e-04,  4.27251199e-02, -1.34221053e-03, -9.78046855e-02,\n",
       "        3.05445261e-03,  4.35635947e+00,  8.90746237e+00,  9.54401200e-02,\n",
       "        9.04533260e-02,  1.09705672e-02,  1.69755818e-04,  9.77440278e-03,\n",
       "       -3.54790130e-04, -5.02470996e-02,  1.75551967e-03,  4.39996214e-01,\n",
       "        2.26194671e+00,  5.81292177e-02,  4.29646789e-02,  6.51220250e-02,\n",
       "        5.80889417e-04,  8.63263977e-03,  1.58756505e-04, -2.91065012e-01,\n",
       "       -4.52640715e-04,  1.96299323e-01,  6.61619413e+00,  2.72599949e-01,\n",
       "        2.31594969e-01,  5.48173694e-02, -1.10165227e-02, -2.15081603e-02,\n",
       "       -8.05523997e-02, -1.14986334e-01, -4.56016698e-01, -8.55144168e-01,\n",
       "        4.75175093e+00,  9.30058281e-01,  9.08257704e-01,  2.96113625e-02,\n",
       "       -7.45843650e-03, -4.52459751e-02, -1.45304611e-01, -7.97117497e-02,\n",
       "       -2.57757649e-01, -1.55134593e+00,  2.75569617e+00,  1.04849763e+00,\n",
       "        1.03433793e+00,  2.16319803e-02, -5.27336804e-03, -4.24042258e-02,\n",
       "       -1.27917885e-01, -5.55800809e-02, -1.67664573e-01, -1.34822496e+00,\n",
       "        1.76714587e+00,  2.73620069e-01,  2.33731863e-01,  5.44965578e-02,\n",
       "        1.17204654e-02, -2.20841756e-02,  7.98811510e-02, -1.23115710e-01,\n",
       "        4.52137460e-01, -8.59998452e-01,  4.75175093e+00,  9.31565346e-01,\n",
       "        9.10875724e-01,  2.73534715e-02,  6.24128059e-03, -3.89845480e-02,\n",
       "        1.40209730e-01, -6.91843688e-02,  2.48597865e-01, -1.55417260e+00,\n",
       "        2.75569617e+00,  1.04980295e+00,  1.03650562e+00,  1.94597503e-02,\n",
       "        3.96627251e-03, -3.35788530e-02,  1.21831372e-01, -4.39522220e-02,\n",
       "        1.59468208e-01, -1.35007352e+00,  1.76714587e+00,  4.26043644e-01,\n",
       "        3.33216404e-01,  1.23120699e-01,  3.61190658e-02, -4.96240275e-02,\n",
       "        1.74239507e-01,  1.25795200e-01, -4.12812743e-01,  7.20503636e-01,\n",
       "        1.66108048e+00,  3.17992102e-01,  3.49033327e-01,  1.76170230e-01,\n",
       "        7.75444392e-02, -1.58742037e-01,  1.23212715e-01,  3.49135689e-01,\n",
       "       -2.88453019e-01,  5.44136086e-01,  1.22954019e+00,  4.28347842e-01,\n",
       "        3.30492074e-01,  1.23123359e-01, -3.26157175e-02, -4.31926088e-02,\n",
       "       -1.75926261e-01,  1.10728637e-01,  4.17346697e-01,  7.20123919e-01,\n",
       "        1.66108048e+00,  3.21184385e-01,  3.41993031e-01,  1.77012662e-01,\n",
       "       -7.87837637e-02, -1.54162709e-01, -1.28254775e-01,  3.39987737e-01,\n",
       "        3.00797120e-01,  5.41540989e-01,  1.22954019e+00, -6.84076360e-03,\n",
       "        9.21053669e-03,  9.50470552e-03, -4.15787824e-03,  3.59272395e-03,\n",
       "        9.69593440e-03, -6.62465076e-03,  1.50571617e-03,  1.57337414e-02,\n",
       "       -2.15429938e-03,  3.78276537e-03,  9.86148745e-03,  1.16677185e-03,\n",
       "        1.67272721e-03,  1.56802586e-02, -2.17597989e-03,  4.79072981e-03,\n",
       "        9.85062665e-03, -2.38936807e-03,  3.80244570e-03,  2.45350083e-02,\n",
       "       -3.03875599e-03,  5.02198138e-03,  9.44850876e-03, -2.33364155e-03,\n",
       "        6.44002482e-04,  2.45555066e-02, -4.28184579e-03,  5.00063106e-03,\n",
       "        9.53823858e-03, -2.33364155e-03,  6.44002482e-04,  2.45555066e-02,\n",
       "       -4.28184579e-03,  5.00063106e-03,  9.53823858e-03,  4.75286113e-03,\n",
       "        4.76476840e-03,  1.01804342e-02, -2.69266720e-03,  4.60606018e-03,\n",
       "        9.40990477e-03,  4.95768904e-03, -4.49700968e-03,  1.01793688e-02,\n",
       "       -6.34177039e-03,  4.52533398e-03,  9.62580020e-03,  4.95768904e-03,\n",
       "       -4.49700968e-03,  1.01793688e-02, -6.34177039e-03,  4.52533398e-03,\n",
       "        9.62580020e-03, -7.97208252e-03,  7.60038901e-03,  9.95661037e-03,\n",
       "       -3.40690821e-03,  3.01353006e-03,  9.51226499e-03, -7.99880208e-03,\n",
       "        8.70075877e-03,  8.84803833e-03, -3.38899939e-03,  3.20060734e-03,\n",
       "        9.69752644e-03, -1.73650502e-03,  1.09479898e-02,  1.61065282e-02,\n",
       "       -3.92784290e-03,  6.29366210e-03,  8.80725357e-03, -1.73088570e-03,\n",
       "        6.65053052e-03,  1.18288850e-02, -4.00856688e-03,  6.99671144e-03,\n",
       "        8.10084133e-03,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aff0a3e-a047-4da7-aead-28470aad0f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-0.4, 0.4, (17,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d19bf1-44ae-4628-909d-8a529b9077e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05293579, -0.02909377,  0.12628955,  0.15863812, -0.14441639,\n",
       "       -0.0146535 , -0.04183137,  0.30172417, -0.05358713, -0.20124765,\n",
       "       -0.3164999 , -0.08170177, -0.13806766, -0.0086144 , -0.1124486 ,\n",
       "       -0.22309826,  0.36465538], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "865db760-7704-44a3-a5b8-7717a332cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 1 - Step:100, Reward:4.7, Total:467.3\n",
      "EPISODE 1 - Step:200, Reward:4.9, Total:951.0\n"
     ]
    }
   ],
   "source": [
    "# RANDOM ACTIONS\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if the humanoid falls or the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(240):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action = env.action_space.sample() #random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() #render physics step (CPU speed = 0.1 seconds)\n",
    "    time.sleep(1/240) #slow down to real-time (240 steps × 1/240 second sleep = 1 second)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05289fe9-8ac4-4158-b76f-23f4c9cf3b64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58dc0b4-5b6e-402d-b8e6-c732aba106bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WindowViewer.__del__ at 0x7fd4ce8e15f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 335, in __del__\n",
      "    self.free()\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 330, in free\n",
      "    glfw.destroy_window(self.window)\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/glfw/__init__.py\", line 1282, in destroy_window\n",
      "    window_addr = ctypes.cast(ctypes.pointer(window),\n",
      "TypeError: _type_ must have storage info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:4.7, Total:4.7\n",
      "EPISODE 1 - Step:20, Reward:5.6, Total:96.8\n",
      "------------------------------------------\n",
      "EPISODE 2 - Step:1, Reward:4.6, Total:4.6\n",
      "EPISODE 2 - Step:37, Reward:5.0, Total:187.5\n",
      "------------------------------------------\n",
      "EPISODE 3 - Step:1, Reward:4.7, Total:4.7\n",
      "EPISODE 3 - Step:53, Reward:5.9, Total:279.8\n",
      "------------------------------------------\n",
      "EPISODE 4 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 4 - Step:33, Reward:6.2, Total:179.3\n",
      "------------------------------------------\n",
      "EPISODE 5 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 5 - Step:37, Reward:6.2, Total:203.6\n",
      "------------------------------------------\n",
      "EPISODE 6 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 6 - Step:42, Reward:6.2, Total:230.4\n",
      "------------------------------------------\n",
      "EPISODE 7 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 7 - Step:41, Reward:6.3, Total:224.2\n",
      "------------------------------------------\n",
      "EPISODE 8 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 8 - Step:34, Reward:6.2, Total:186.7\n",
      "------------------------------------------\n",
      "EPISODE 9 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 9 - Step:38, Reward:6.3, Total:211.8\n",
      "------------------------------------------\n",
      "EPISODE 10 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 10 - Step:36, Reward:6.2, Total:197.7\n",
      "------------------------------------------\n",
      "EPISODE 11 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 11 - Step:42, Reward:5.9, Total:217.9\n",
      "------------------------------------------\n",
      "EPISODE 12 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 12 - Step:34, Reward:6.1, Total:182.4\n",
      "------------------------------------------\n",
      "EPISODE 13 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 13 - Step:34, Reward:6.3, Total:186.9\n",
      "------------------------------------------\n",
      "EPISODE 14 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 14 - Step:38, Reward:6.4, Total:211.3\n",
      "------------------------------------------\n",
      "EPISODE 15 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 15 - Step:36, Reward:6.4, Total:200.9\n",
      "------------------------------------------\n",
      "EPISODE 16 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 16 - Step:37, Reward:6.4, Total:204.9\n",
      "------------------------------------------\n",
      "EPISODE 17 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 17 - Step:37, Reward:6.4, Total:204.3\n",
      "------------------------------------------\n",
      "EPISODE 18 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 18 - Step:37, Reward:6.2, Total:201.1\n",
      "------------------------------------------\n",
      "EPISODE 19 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 19 - Step:36, Reward:6.1, Total:193.5\n",
      "------------------------------------------\n",
      "EPISODE 20 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 20 - Step:42, Reward:6.2, Total:230.0\n",
      "------------------------------------------\n",
      "EPISODE 21 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 21 - Step:39, Reward:6.2, Total:213.2\n",
      "------------------------------------------\n",
      "EPISODE 22 - Step:1, Reward:4.9, Total:4.9\n",
      "EPISODE 22 - Step:40, Reward:6.2, Total:217.3\n",
      "------------------------------------------\n",
      "EPISODE 23 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 23 - Step:50, Reward:5.7, Total:257.6\n",
      "------------------------------------------\n",
      "EPISODE 24 - Step:1, Reward:5.0, Total:5.0\n",
      "EPISODE 24 - Step:40, Reward:5.1, Total:198.2\n",
      "------------------------------------------\n",
      "EPISODE 25 - Step:1, Reward:4.9, Total:4.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_z/cl6cx94516g2tlbhzcbl4nfc0000gn/T/ipykernel_63924/848025175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mexploration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add random noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_action\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m## reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/humanoid_v4.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         return self.mujoco_renderer.render(\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         )\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, render_mode, camera_id, camera_name)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_viewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                 \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    392\u001b[0m                     )\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mglfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m             \u001b[0mglfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             self._time_per_render = 0.9 * self._time_per_render + 0.1 * (\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/glfw/__init__.py\u001b[0m in \u001b[0;36mswap_buffers\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   2378\u001b[0m         \u001b[0mvoid\u001b[0m \u001b[0mglfwSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLFWwindow\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m     \"\"\"\n\u001b[0;32m-> 2380\u001b[0;31m     \u001b[0m_glfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglfwSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m \u001b[0m_glfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglfwSwapInterval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/glfw/__init__.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, *args)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0m_callback_exception_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \"\"\"\n\u001b[0;32m--> 687\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_exc_info_from_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_exc_info_from_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = True #reset if the humanoid falls or the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "exploration_rate = 0.5 #start wild\n",
    "preferred_action = np.zeros(env.action_space.shape) #knowledge to update with experience\n",
    "\n",
    "for _ in range(1000):\n",
    "    ## action\n",
    "    step += 1\n",
    "    exploration = np.random.normal(loc=0, scale=exploration_rate, size=env.action_space.shape) #add random noise\n",
    "    action = np.clip(a=preferred_action+exploration, a_min=-1, a_max=1)\n",
    "    obs, reward, terminated, truncated, info = env.step(action) \n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    if reward > 0:\n",
    "        preferred_action += (action-preferred_action)*0.05 #learning_rate\n",
    "    exploration_rate = max(0.05, exploration_rate*0.99) #min_exploration=0.05, decay_exploration=0.99\n",
    "    ## render\n",
    "    env.render() \n",
    "    time.sleep(1/240)\n",
    "    if (step == 1) or (step % 100 == 0):\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated:\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8942f3a-53fc-4cb3-b1e5-f762ef5fc827",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb8d555-7b8d-4aff-9575-67d36e392ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WindowViewer.__del__ at 0x7fef22e7e0e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 335, in __del__\n",
      "    self.free()\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 330, in free\n",
      "    glfw.destroy_window(self.window)\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TORCH/lib/python3.7/site-packages/glfw/__init__.py\", line 1282, in destroy_window\n",
      "    window_addr = ctypes.cast(ctypes.pointer(window),\n",
      "TypeError: _type_ must have storage info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DONE\n"
     ]
    }
   ],
   "source": [
    "#pip install torch\n",
    "#pip install stable-baselines3\n",
    "#pip install tensorboard\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# TRAIN\n",
    "env = gym.make(\"Humanoid-v4\") #no rendering to speed up\n",
    "env = DummyVecEnv([lambda:env])\n",
    "\n",
    "print(\"Training START\")\n",
    "model = PPO(policy=\"MlpPolicy\", env=env, verbose=0, \n",
    "            learning_rate=0.005, ent_coef=0.005, #exploration\n",
    "            tensorboard_log=\"logs/\") #>tensorboard --logdir=logs/\n",
    "model.learn(total_timesteps=3_000_000, #1h\n",
    "            tb_log_name=\"model_humanoid\", log_interval=10)\n",
    "print(\"Training DONE\")\n",
    "\n",
    "model.save(\"model_humanoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a132979e-2f8d-4cb6-a905-0071520f5f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:4.7, Total:4.7\n",
      "EPISODE 1 - Step:100, Reward:5.8, Total:509.3\n",
      "EPISODE 1 - Step:200, Reward:4.6, Total:1014.7\n",
      "EPISODE 1 - Step:300, Reward:4.9, Total:1478.7\n",
      "EPISODE 1 - Step:400, Reward:4.6, Total:1941.6\n",
      "EPISODE 1 - Step:500, Reward:4.7, Total:2404.6\n",
      "EPISODE 1 - Step:600, Reward:4.7, Total:2879.2\n",
      "EPISODE 1 - Step:700, Reward:4.6, Total:3344.9\n",
      "EPISODE 1 - Step:800, Reward:4.5, Total:3816.8\n",
      "EPISODE 1 - Step:900, Reward:4.7, Total:4287.6\n",
      "EPISODE 1 - Step:1000, Reward:4.5, Total:4749.2\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\n",
    "model = PPO.load(path=\"model_humanoid\", env=env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if the humanoid falls or the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action, _ = model.predict(obs)    \n",
    "    obs, reward, terminated, truncated, info = env.step(action) \n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() \n",
    "    time.sleep(1/240)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b667fe",
   "metadata": {},
   "source": [
    "### 2-Ant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a1866",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866bec37-65ed-42c0-bc59-902f557ddc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFO: 0 ---\n",
      "{} \n",
      "\n",
      "--- OBS: (27,) ---\n",
      "[ 6.91233009e-01  9.95281615e-01 -5.14949293e-02 -3.03821511e-04\n",
      "  8.22355581e-02  8.73588219e-05  9.58393541e-02  4.86540090e-02\n",
      " -3.69887489e-02 -5.59380104e-02 -6.02988691e-02  4.11670342e-03\n",
      "  1.01618978e-02 -1.63464515e-01 -1.29829693e-01 -3.18777907e-02\n",
      "  2.00860858e-02  6.09582957e-02  5.64071545e-02  1.13314746e-01\n",
      " -1.01847353e-01 -5.93092945e-02  2.77926702e-02 -1.58043671e-01\n",
      "  6.86104336e-02 -2.15485311e-02 -8.70702648e-02] \n",
      "\n",
      "--- ACTIONS: Box(-1.0, 1.0, (8,), float32) ---\n",
      "[-0.05835706  0.39210606  0.5151652  -0.3944796   0.3655811   0.0078584\n",
      "  0.7871264   0.7373914 ] \n",
      "\n",
      "--- REWARD ---\n",
      "-0.0998763749711502 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"Ant-v4\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(f\"--- INFO: {len(info)} ---\")\n",
    "print(info, \"\\n\")\n",
    "\n",
    "print(f\"--- OBS: {obs.shape} ---\")\n",
    "print(obs, \"\\n\")\n",
    "\n",
    "print(f\"--- ACTIONS: {env.action_space} ---\")\n",
    "print(env.action_space.sample(), \"\\n\")\n",
    "\n",
    "print(f\"--- REWARD ---\")\n",
    "obs, reward, terminated, truncated, info = env.step( env.action_space.sample() )\n",
    "print(reward, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203263a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:-0.2, Total:-0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/glfw/__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:100, Reward:-0.1, Total:-29.6\n",
      "EPISODE 1 - Step:200, Reward:1.8, Total:-8.8\n"
     ]
    }
   ],
   "source": [
    "# RANDOM ACTIONS\n",
    "import time\n",
    "\n",
    "env = gym.make(\"Ant-v4\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if the humanoid falls or the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(240):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action = env.action_space.sample() #random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() #render physics step (CPU speed = 0.1 seconds)\n",
    "    time.sleep(1/240) #slow down to real-time (240 steps × 1/240 second sleep = 1 second)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e098b5-7c9f-4377-b7cd-d675d9ee7ee6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Custom Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33d76da-b0ef-42df-9341-d2718786504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/gymnasium/envs/mujoco/assets/ant.xml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.join(os.path.dirname(gym.__file__), \"envs/mujoco/assets/ant.xml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214ab046-9d52-48bf-aa96-f7732f3d12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.mujoco.ant_v4 import AntEnv\n",
    "from gymnasium.envs.registration import register\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## modify the class\n",
    "class CustomAntEnv(AntEnv):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(xml_file=os.getcwd()+\"/assets/custom_ant.xml\", **kwargs)\n",
    "        #self.target_height = 0.8\n",
    "\n",
    "    def CUSTOM_REWARD(self, action, info):\n",
    "        torso_height = float(self.data.qpos[2]) #torso z-coordinate = how high it is\n",
    "        reward = np.clip(a=torso_height-0.6, a_min=0, a_max=1) *10 #when the torso is high\n",
    "        terminated = bool(torso_height < 0.2 ) #if torso close to the ground\n",
    "        info[\"torso_height\"] = torso_height #add info for logging\n",
    "        return reward, terminated, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action) #override original step()\n",
    "        new_reward, new_terminated, new_info = self.CUSTOM_REWARD(action, info)\n",
    "        return obs, new_reward, new_terminated, truncated, new_info #must return the same things\n",
    "\n",
    "    def reset_model(self):\n",
    "        return super().reset_model() #keeping the reset as it is\n",
    "\n",
    "## register the new env\n",
    "register(id=\"CustomAntEnv-v1\", entry_point=\"__main__:CustomAntEnv\")\n",
    "\n",
    "## test\n",
    "env = gym.make(\"CustomAntEnv-v1\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cda11-25e3-4f85-a0db-052db5445751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb521257-cf55-4547-8d9e-6921dff2dde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFO: 0 ---\n",
      "{} \n",
      "\n",
      "--- OBS: (27,) ---\n",
      "[ 0.79430009  0.99394779 -0.08066536 -0.02363279  0.07072752  0.07002416\n",
      "  0.01029792  0.07577632  0.03563821  0.07026008  0.08391228  0.03408507\n",
      "  0.01852145  0.12401217 -0.09310116 -0.22491088 -0.10223741 -0.03463083\n",
      " -0.21741644  0.07755763 -0.10719397  0.07505244 -0.02819262  0.13671288\n",
      " -0.05881673  0.11423527 -0.06173078] \n",
      "\n",
      "--- ACTIONS: Discrete(5) ---\n",
      "0 \n",
      "\n",
      "--- REWARD ---\n",
      "2.0754143283635464 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DISCRETIZE THE ENV\n",
    "class DiscreteEnvWrapper(gym.Env):\n",
    "    \n",
    "    def __init__(self, render_mode=None): \n",
    "        super().__init__() \n",
    "        self.env = gym.make(\"CustomAntEnv-v1\", render_mode=render_mode) \n",
    "        self.action_space = gym.spaces.Discrete(5)  #will have 5 actions \n",
    "        self.observation_space = self.env.observation_space #same observation space\n",
    "        n_joints = self.env.action_space.shape[0]         \n",
    "        self.action_map = [\n",
    "            ## action 0 = stand still \n",
    "            np.zeros(n_joints),\n",
    "            ## action 1 = push all forward\n",
    "            0.5*np.ones(n_joints),\n",
    "            ## action 2 = push all backward\n",
    "           -0.5*np.ones(n_joints),\n",
    "            ## action 3 = front legs forward + back legs backward \n",
    "            0.5*np.concatenate([np.ones(n_joints//2), -np.ones(n_joints//2)]),\n",
    "            ## action 4 = front legs backward + back legs forward \n",
    "            0.5*np.concatenate([-np.ones(n_joints//2), np.ones(n_joints//2)])\n",
    "        ] \n",
    "        \n",
    "    def step(self, discrete_action): \n",
    "        assert self.action_space.contains(discrete_action) \n",
    "        continuous_action = self.action_map[discrete_action] \n",
    "        obs, reward, terminated, truncated, info = self.env.step(continuous_action) \n",
    "        return obs, reward, terminated, truncated, info\n",
    "        \n",
    "    def reset(self, **kwargs): \n",
    "        obs, info = self.env.reset(**kwargs) \n",
    "        return obs, info \n",
    "    \n",
    "    def render(self): \n",
    "        return self.env.render() \n",
    "    \n",
    "    def close(self): \n",
    "        self.env.close()\n",
    "\n",
    "## test\n",
    "env = DiscreteEnvWrapper()\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(f\"--- INFO: {len(info)} ---\")\n",
    "print(info, \"\\n\")\n",
    "\n",
    "print(f\"--- OBS: {obs.shape} ---\")\n",
    "print(obs, \"\\n\")\n",
    "\n",
    "print(f\"--- ACTIONS: {env.action_space} ---\")\n",
    "discrete_action = env.action_space.sample()\n",
    "continuous_action = env.action_map[discrete_action] \n",
    "print(\"discrete:\", discrete_action, \"-> continuous:\", continuous_action, \"\\n\")\n",
    "\n",
    "print(f\"--- REWARD ---\")\n",
    "obs, reward, terminated, truncated, info = env.step( discrete_action )\n",
    "print(reward, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5751ce07-08ba-42cb-959b-8d6b47aefdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training START\n",
      "Training DONE\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3 as sb\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# TRAIN\n",
    "env = DiscreteEnvWrapper(render_mode=None) #no rendering to speed up\n",
    "env = DummyVecEnv([lambda:env]) \n",
    "model_name = \"ant_dqn\"\n",
    "\n",
    "print(\"Training START\")\n",
    "model = sb.DQN(policy=\"MlpPolicy\", env=env, verbose=0, learning_rate=0.005,\n",
    "               exploration_fraction=0.2, exploration_final_eps=0.05, #eps decays linearly from 1 to 0.05\n",
    "               tensorboard_log=\"logs/\") #>tensorboard --logdir=logs/\n",
    "model.learn(total_timesteps=1_000_000, #1h\n",
    "            tb_log_name=model_name, log_interval=10)\n",
    "print(\"Training DONE\")\n",
    "\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9ebee11-d8eb-40cc-a11c-cbde4e135d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:0.9, Total:0.9\n",
      "EPISODE 1 - Step:100, Reward:0.0, Total:1.9\n",
      "EPISODE 1 - Step:200, Reward:0.0, Total:60.0\n",
      "EPISODE 1 - Step:300, Reward:0.0, Total:121.8\n",
      "EPISODE 1 - Step:400, Reward:0.0, Total:198.8\n",
      "EPISODE 1 - Step:500, Reward:0.0, Total:267.8\n",
      "EPISODE 1 - Step:600, Reward:0.0, Total:325.7\n",
      "EPISODE 1 - Step:700, Reward:0.0, Total:325.7\n",
      "EPISODE 1 - Step:800, Reward:2.8, Total:437.4\n",
      "EPISODE 1 - Step:900, Reward:0.0, Total:527.5\n",
      "EPISODE 1 - Step:1000, Reward:0.0, Total:527.5\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "env = DiscreteEnvWrapper(render_mode=\"human\")\n",
    "model = sb.DQN.load(path=model_name, env=env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action, _ = model.predict(obs)    \n",
    "    obs, reward, terminated, truncated, info = env.step(action) \n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() \n",
    "    time.sleep(1/240)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8d71c-f233-4bc3-a75c-d9f190b30fe9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "309d2cd6-5237-4839-b1b3-d1c5fbcf6045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training START\n",
      "Training DONE\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "env_name, model_name = \"CustomAntEnv-v1\", \"ant_sac\"\n",
    "env = gym.make(env_name) #no rendering to speed up\n",
    "env = DummyVecEnv([lambda:env])\n",
    "\n",
    "print(\"Training START\")\n",
    "model = sb.SAC(policy=\"MlpPolicy\", env=env, verbose=0, learning_rate=0.005, \n",
    "                ent_coef=0.005, #exploration\n",
    "                tensorboard_log=\"logs/\") #>tensorboard --logdir=logs/\n",
    "model.learn(total_timesteps=100_000, #3h\n",
    "            tb_log_name=model_name, log_interval=10)\n",
    "print(\"Training DONE\")\n",
    "\n",
    "## save\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ef8e07-337d-429d-a0ad-3aa315222caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WindowViewer.__del__ at 0x7fb2ef249bd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 335, in __del__\n",
      "    self.free()\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 330, in free\n",
      "    glfw.destroy_window(self.window)\n",
      "  File \"/Users/mdp/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/glfw/__init__.py\", line 1282, in destroy_window\n",
      "    window_addr = ctypes.cast(ctypes.pointer(window),\n",
      "TypeError: _type_ must have storage info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 - Step:1, Reward:1.3, Total:1.3\n",
      "EPISODE 1 - Step:100, Reward:10.0, Total:785.3\n",
      "EPISODE 1 - Step:200, Reward:10.0, Total:1729.2\n",
      "EPISODE 1 - Step:300, Reward:4.3, Total:2412.0\n",
      "EPISODE 1 - Step:400, Reward:10.0, Total:3257.6\n",
      "EPISODE 1 - Step:500, Reward:4.2, Total:4059.2\n",
      "EPISODE 1 - Step:600, Reward:4.0, Total:4926.7\n",
      "EPISODE 1 - Step:700, Reward:10.0, Total:5802.7\n",
      "EPISODE 1 - Step:800, Reward:4.4, Total:6678.0\n",
      "EPISODE 1 - Step:900, Reward:5.4, Total:7430.4\n",
      "EPISODE 1 - Step:1000, Reward:10.0, Total:8311.7\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "model = sb.SAC.load(path=model_name, env=env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "reset = False #reset if the humanoid falls or the episode ends\n",
    "episode = 1\n",
    "total_reward, step = 0, 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    ## action\n",
    "    step += 1\n",
    "    action, _ = model.predict(obs)    \n",
    "    obs, reward, terminated, truncated, info = env.step(action) \n",
    "    ## reward\n",
    "    total_reward += reward\n",
    "    ## render\n",
    "    env.render() \n",
    "    time.sleep(1/240)\n",
    "    if (step == 1) or (step % 100 == 0): #print first step and every 100 steps\n",
    "        print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "    ## reset\n",
    "    if reset:\n",
    "        if terminated or truncated: #print the last step\n",
    "            print(f\"EPISODE {episode} - Step:{step}, Reward:{reward:.1f}, Total:{total_reward:.1f}\")\n",
    "            obs, info = env.reset()\n",
    "            episode += 1\n",
    "            total_reward, step = 0, 0\n",
    "            print(\"------------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f62013-4405-4c61-a57a-1293f93ed501",
   "metadata": {},
   "source": [
    "###### Evolutionary Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "556165bb-30ac-454c-9a08-a6b7320c07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-09 14:14:15] INFO     <21804> evotorch.core: Instance of `GymNE` (id:140399605920960) -- The `dtype` for the problem's decision variables is set as torch.float32\n",
      "[2025-11-09 14:14:15] INFO     <21804> evotorch.core: Instance of `GymNE` (id:140399605920960) -- `eval_dtype` (the dtype of the fitnesses and evaluation data) is set as torch.float32\n",
      "[2025-11-09 14:14:15] INFO     <21804> evotorch.core: Instance of `GymNE` (id:140399605920960) -- The `device` of the problem is set as cpu\n",
      "[2025-11-09 14:14:15] INFO     <21804> evotorch.core: Instance of `GymNE` (id:140399605920960) -- The number of actors that will be allocated for parallelized evaluation is 8\n",
      "[2025-11-09 14:14:15] INFO     <21804> evotorch.core: Instance of `GymNE` (id:140399605920960) -- Number of GPUs that will be allocated per actor is None\n",
      "[2025-11-09 14:14:15] INFO     <21804> evotorch.optimizers: Instance of `ClipUp` (id:140399581695808) -- The maximum speed for the ClipUp optimizer is set as 0.01 which is two times the given step size.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m## train\u001b[39;00m\n\u001b[1;32m     19\u001b[0m StdOutLogger(searcher\u001b[38;5;241m=\u001b[39mmodel, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/evotorch/algorithms/searchalgorithm.py:425\u001b[0m, in \u001b[0;36mSearchAlgorithm.run\u001b[0;34m(self, num_generations, reset_first_step_datetime)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_first_step_datetime()\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(num_generations)):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_of_run_hook) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_of_run_hook(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/evotorch/algorithms/searchalgorithm.py:390\u001b[0m, in \u001b[0;36mSearchAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_step_datetime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_step_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_status({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_count})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/evotorch/algorithms/distributed/gaussian.py:354\u001b[0m, in \u001b[0;36mGaussianSearchAlgorithm._step_non_distributed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_population \u001b[38;5;241m=\u001b[39m SolutionBatch\u001b[38;5;241m.\u001b[39mcat(populations)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_iter:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# If we are computing the first generation, we just sample from our distribution and evaluate\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;66;03m# the solutions.\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[43mfill_and_eval_pop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# If we are computing next generations, then we need to compute the gradients of the last\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# generation, sample a new population, and evaluate the new population's solutions.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/evotorch/algorithms/distributed/gaussian.py:295\u001b[0m, in \u001b[0;36mGaussianSearchAlgorithm._step_non_distributed.<locals>.fill_and_eval_pop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution\u001b[38;5;241m.\u001b[39msample(out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_population\u001b[38;5;241m.\u001b[39maccess_values(), generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem)\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# Finally, here, the solutions are evaluated.\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_population\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# If num_interactions is not None, then this means that we have a threshold for the number\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# of simulator interactions to reach before declaring the phase of sampling complete.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# Therefore, to properly count the simulator interactions we made during this generation, we need\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# to get the interaction count before starting our sampling and evaluation operations.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     first_num_interactions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_interaction_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/evotorch/core.py:2559\u001b[0m, in \u001b[0;36mProblem.evaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2555\u001b[0m must_sync_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sync_before()\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_preparations()\n\u001b[0;32m-> 2559\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_sync_after:\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sync_after()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/evotorch/core.py:2598\u001b[0m, in \u001b[0;36mProblem._evaluate_all\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   2590\u001b[0m \u001b[38;5;66;03m# mapresult = self._actor_pool.map(lambda a, v: a.evaluate_batch.remote(v), list(pieces))\u001b[39;00m\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;66;03m# for i, evals in enumerate(mapresult):\u001b[39;00m\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;66;03m#    row_begin, row_end = pieces.indices_of(i)\u001b[39;00m\n\u001b[1;32m   2593\u001b[0m \u001b[38;5;66;03m#    batch._evdata[row_begin:row_end, :] = evals\u001b[39;00m\n\u001b[1;32m   2595\u001b[0m mapresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_pool\u001b[38;5;241m.\u001b[39mmap_unordered(\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m a, v: a\u001b[38;5;241m.\u001b[39mevaluate_batch_piece\u001b[38;5;241m.\u001b[39mremote(v[\u001b[38;5;241m0\u001b[39m], v[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(pieces))\n\u001b[1;32m   2597\u001b[0m )\n\u001b[0;32m-> 2598\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, evals \u001b[38;5;129;01min\u001b[39;00m mapresult:\n\u001b[1;32m   2599\u001b[0m     row_begin, row_end \u001b[38;5;241m=\u001b[39m pieces\u001b[38;5;241m.\u001b[39mindices_of(i)\n\u001b[1;32m   2600\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_evdata[row_begin:row_end, :] \u001b[38;5;241m=\u001b[39m evals\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/ray/util/actor_pool.py:170\u001b[0m, in \u001b[0;36mActorPool.map_unordered.<locals>.get_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_generator\u001b[39m():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_next():\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/ray/util/actor_pool.py:352\u001b[0m, in \u001b[0;36mActorPool.get_next_unordered\u001b[0;34m(self, timeout, ignore_if_timedout)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo more results to get\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# TODO(ekl) bulk wait for performance\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_future_to_actor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m timeout_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed out waiting for result\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m raise_timeout_after_ignore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:22\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     21\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:104\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/ray/_private/worker.py:3113\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   3111\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   3112\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 3113\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3486\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/includes/common.pxi:96\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pip install evotorch\n",
    "\n",
    "from evotorch.neuroevolution import GymNE\n",
    "from evotorch.algorithms import PGPE\n",
    "from evotorch.logging import StdOutLogger\n",
    "\n",
    "## problem\n",
    "problem = GymNE(env=CustomAntEnv, #directly the class because it's custom env\n",
    "                network=\"Linear(obs_length, act_length)\", #linear policy\n",
    "                observation_normalization=True,\n",
    "                decrease_rewards_by=1, #normalization trick to stabilize evolution\n",
    "                num_actors=\"max\") #use all available CPU cores\n",
    "\n",
    "## model\n",
    "model = PGPE(problem, popsize=100, center_learning_rate=0.005, \n",
    "             stdev_init=0.1, stdev_learning_rate=0.1)\n",
    "\n",
    "## train\n",
    "StdOutLogger(searcher=model, interval=10)\n",
    "model.run(num_generations=1) #1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7591d9c9-e607-469a-ad93-be21a7667463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mPGPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproblem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevotorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProblem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpopsize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcenter_learning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstdev_learning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstdev_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mradius_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_interactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpopsize_max\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'clipup'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mranking_method\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'centered'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcenter_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstdev_min\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstdev_max\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstdev_max_change\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msymmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobj_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdistributed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpopsize_weighted_grad_avg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "PGPE: Policy gradient with parameter-based exploration.\n",
       "\n",
       "This implementation is the symmetric-sampling variant proposed\n",
       "in the paper Sehnke et al. (2010).\n",
       "\n",
       "Inspired by the PGPE implementations used in the studies\n",
       "of Ha (2017, 2019), and by the evolution strategy variant of\n",
       "Salimans et al. (2017), this PGPE implementation uses 0-centered\n",
       "ranking by default.\n",
       "The default optimizer for this PGPE implementation is ClipUp\n",
       "(Toklu et al., 2020).\n",
       "\n",
       "References:\n",
       "\n",
       "    Frank Sehnke, Christian Osendorfer, Thomas Ruckstiess,\n",
       "    Alex Graves, Jan Peters, Jurgen Schmidhuber (2010).\n",
       "    Parameter-exploring Policy Gradients.\n",
       "    Neural Networks 23(4), 551-559.\n",
       "\n",
       "    David Ha (2017). Evolving Stable Strategies.\n",
       "    <http://blog.otoro.net/2017/11/12/evolving-stable-strategies/>\n",
       "\n",
       "    Salimans, T., Ho, J., Chen, X., Sidor, S. and Sutskever, I. (2017).\n",
       "    Evolution Strategies as a Scalable Alternative to\n",
       "    Reinforcement Learning.\n",
       "\n",
       "    David Ha (2019). Reinforcement Learning for Improving Agent Design.\n",
       "    Artificial life 25 (4), 352-365.\n",
       "\n",
       "    Toklu, N.E., Liskowski, P., Srivastava, R.K. (2020).\n",
       "    ClipUp: A Simple and Powerful Optimizer\n",
       "    for Distribution-based Policy Evolution.\n",
       "    Parallel Problem Solving from Nature (PPSN 2020).\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "`__init__(...)`: Initialize the PGPE algorithm.\n",
       "\n",
       "Args:\n",
       "    problem: The problem object which is being worked on.\n",
       "        The problem must have its dtype defined\n",
       "        (which means it works on Solution objects,\n",
       "        not with custom Solution objects).\n",
       "        Also, the problem must be single-objective.\n",
       "    popsize: The population size.\n",
       "        In the case of PGPE, `popsize` is expected as an even number\n",
       "        in non-distributed mode. In distributed mode, PGPE will\n",
       "        ensure that each sub-population size assigned to a remote\n",
       "        actor is an even number.\n",
       "        This behavior is because PGPE does symmetric sampling\n",
       "        (i.e. solutions are sampled in pairs).\n",
       "    center_learning_rate: The learning rate for the center\n",
       "        of the search distribution.\n",
       "    stdev_learning_rate: The learning rate for the standard\n",
       "        deviation values of the search distribution.\n",
       "    stdev_init: The initial standard deviation of the search\n",
       "        distribution, expressed as a scalar or as an array.\n",
       "        Determines the initial coverage area of the search\n",
       "        distribution.\n",
       "        If one wishes to configure the coverage area via the\n",
       "        argument `radius_init` instead, then `stdev_init` is expected\n",
       "        as None.\n",
       "    radius_init: The initial radius of the search distribution,\n",
       "        expressed as a scalar.\n",
       "        Determines the initial coverage area of the search\n",
       "        distribution.\n",
       "        Here, \"radius\" is defined as the norm of the search\n",
       "        distribution.\n",
       "        If one wishes to configure the coverage area via the\n",
       "        argument `stdev_init` instead, then `radius_init` is expected\n",
       "        as None.\n",
       "    num_interactions: When given as an integer n,\n",
       "        it is ensured that a population has interacted with\n",
       "        the GymProblem's environment n times. If this target\n",
       "        has not been reached yet, then the population is declared\n",
       "        too small, and gets extended with more samples,\n",
       "        until n amount of interactions is reached.\n",
       "        When given as None, popsize is the only configuration\n",
       "        affecting the size of a population.\n",
       "    popsize_max: Having `num_interactions` set as an integer\n",
       "        might cause the effective population size jump to\n",
       "        unnecesarily large numbers. To prevent this,\n",
       "        one can set `popsize_max` to specify an upper\n",
       "        bound for the effective population size.\n",
       "    optimizer: The optimizer to be used while following the\n",
       "        estimated the gradients.\n",
       "        Can be given as None if a momentum-based optimizer\n",
       "        is not required.\n",
       "        Otherwise, can be given as a str containing the name\n",
       "        of the optimizer (e.g. 'adam', 'clipup');\n",
       "        or as an instance of evotorch.optimizers.TorchOptimizer\n",
       "        or evotorch.optimizers.ClipUp.\n",
       "        The default is 'clipup'.\n",
       "        Note that, for ClipUp, the default maximum speed is set\n",
       "        as twice the given `center_learning_rate`.\n",
       "        This maximum speed can be configured by passing\n",
       "        `{\"max_speed\": ...}` to `optimizer_config`.\n",
       "    optimizer_config: Configuration which will be passed\n",
       "        to the optimizer as keyword arguments.\n",
       "        See `evotorch.optimizers` for details about\n",
       "        which optimizer accepts which keyword arguments.\n",
       "    ranking_method: Which ranking method will be used for\n",
       "        fitness shaping. See the documentation of\n",
       "        `evotorch.ranking.rank(...)` for details.\n",
       "        As in the study of Salimans et al. (2017),\n",
       "        the default is 'centered'.\n",
       "        Can be given as None if no such ranking is required.\n",
       "    center_init: The initial center solution.\n",
       "        Can be left as None.\n",
       "    stdev_min: Lower bound for the standard deviation value/array.\n",
       "        Can be given as a real number, or as an array of real numbers.\n",
       "    stdev_max: Upper bound for the standard deviation value/array.\n",
       "        Can be given as a real number, or as an array of real numbers.\n",
       "    stdev_max_change: The maximum update ratio allowed on the\n",
       "        standard deviation. Expected as None if no such limiter\n",
       "        is needed, or as a real number within 0.0 and 1.0 otherwise.\n",
       "        Like in the implementation of Ha (2017, 2018),\n",
       "        the default value for this setting is 0.2, meaning that\n",
       "        the update on the standard deviation values can not be\n",
       "        more than 20% of their original values.\n",
       "    symmetric: Whether or not the solutions will be sampled\n",
       "        in a symmetric/mirrored/antithetic manner.\n",
       "        The default is True.\n",
       "    obj_index: Index of the objective according to which the\n",
       "        gradient estimations will be done.\n",
       "        For single-objective problems, this can be left as None.\n",
       "    distributed: Whether or not the gradient computation will\n",
       "        be distributed. If `distributed` is given as False and\n",
       "        the problem is not parallelized, then everything will\n",
       "        be centralized (i.e. the entire computation will happen\n",
       "        in the main process).\n",
       "        If `distributed` is given as False, and the problem\n",
       "        is parallelized, then the population will be created\n",
       "        in the main process and then sent to remote workers\n",
       "        for parallelized evaluation, and then the remote fitnesses\n",
       "        will be collected by the main process again for computing\n",
       "        the search gradients.\n",
       "        If `distributed` is given as True, and the problem\n",
       "        is parallelized, then the search algorithm itself will\n",
       "        be distributed, in the sense that each remote actor will\n",
       "        generate its own population (such that the total population\n",
       "        size across all these actors becomes equal to `popsize`)\n",
       "        and will compute its own gradient, and then the main process\n",
       "        will collect these gradients, compute the averaged gradients\n",
       "        and update the main search distribution.\n",
       "        Non-distributed mode has the advantage of keeping the\n",
       "        population in the main process, which is good when one wishes\n",
       "        to do detailed monitoring during the evolutionary process,\n",
       "        but has the disadvantage of having to pass the solutions to\n",
       "        the remote actors and having to collect fitnesses, which\n",
       "        might result in increased interprocess communication traffic.\n",
       "        On the other hand, while it is not possible to monitor the\n",
       "        population in distributed mode, the distributed mode has the\n",
       "        advantage of significantly reducing the interprocess\n",
       "        communication traffic, since the only things communicated\n",
       "        with the remote actors are the search distributions (not the\n",
       "        solutions) and the gradients.\n",
       "    popsize_weighted_grad_avg: Only to be used in distributed mode.\n",
       "        (where being in distributed mode means `distributed` is given\n",
       "        as True). In distributed mode, each actor remotely samples\n",
       "        its own solution batches and computes its own gradients.\n",
       "        These gradients are then collected, and a final average\n",
       "        gradient is computed.\n",
       "        If `popsize_weighted_grad_avg` is True, then, while averaging\n",
       "        over the gradients, each gradient will have its own weight\n",
       "        that is computed according to how many solutions were sampled\n",
       "        by the actor that produced the gradient.\n",
       "        If `popsize_weighted_grad_avg` is False, then, there will not\n",
       "        be weighted averaging (or, each gradient will have equal\n",
       "        weight).\n",
       "        If `popsize_weighted_grad_avg` is None, then, the gradient\n",
       "        weights will be equal a value for `num_interactions` is given\n",
       "        (because `num_interactions` affects the number of solutions\n",
       "        according to the episode lengths, and popsize-weighting the\n",
       "        gradients could be misleading); and the gradient weights will\n",
       "        be weighted according to the sub-population (i.e. sub-batch)\n",
       "        sizes if `num_interactions` is left as None.\n",
       "        The default value for `popsize_weighted_grad_avg` is None.\n",
       "        When the distributed mode is disabled (i.e. when `distributed`\n",
       "        is False), then the argument `popsize_weighted_grad_avg` is\n",
       "        expected as None.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/envs/TOR/lib/python3.10/site-packages/evotorch/algorithms/distributed/gaussian.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?GymNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a249e4d5-e1ac-4abd-bde9-e19c4a36060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "population_center = model.status[\"center\"] #find best policy\n",
    "policy = problem.to_policy(population_center)\n",
    "problem.visualize(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
